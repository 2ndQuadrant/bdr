NEXT STEPS TO PROGRESS
----

- finish the bdr state implementation
  - move to state file
  - test
  - adopt in join process too

- move node join into manager

- File bug on supervisor doesn't reliably restart manager in timely way
  then ignore it!

- Delay starting messaging until have nodelist and nodegroup (manager)

- replicate replication sets on join

- replicate replication set memberships on change

- write persistence for consensus messages
- write db read-out for consensus messages
- write proper consensus 2pc


COMPAT STUFF
---

bdr.bdr_group_create wrapper around bdr.create_node and bdr.create_node_group

HACKS TO PAY DOWN AND TODOs TO FINISH
----

- Replace most uses of node-names with node-ids or "name (id)" in messages

- Use names not integer values when printing state enums, etc

- Instead of assuming there's an interface with the same name as the node,
  look up the interfaces being used by the join target node for its bdr
  subscriptions to peer nodes and use them.

- Permission checks in BDR internal SQL functions for join, messaging, etc

- Move node join process into manager

- Instead of restarting manager, notice when we add new nodes / remove nodes

- Do message serialization and deserialization for BdrMessage, ConsensusMessage
  using pqformat, not just memcpy'ing the structs.

- Submit messages as msgtype, payload and length, not formed BdrMessage with a bunch
  of unused fields.

- Move the messaging into a separate worker, lifetime managed by the pgl manager,
  and do all the bdr IPC/messaging via it. Fixes issues with long running xacts
  in manager, event loop spam, etc. Then unhack pglogical manager accordingly.

- allow submission of multiple msgs in a loop on shmem so we don't have
  to re-enter event loop with xact held open. can still happen due to
  buffer sizes etc, but less likely. (see README.messaging "future work")

- move bdr_messaging IPC into bdr_consensus,
  unify with bdr_msgbroker_recieve so we have a single pool of
  workers that can accept submits  (see README.messaging "future work")

- Fix submit of messages to manager so it doesn't hang indefinitely if
  manager dies. Need to either get bgworker handle (struct is private, but
  we can cheat), or switch to non-blocking MQs and check for pid change
  in manager PGPROC as found in bdr shmem seg

- Add a log filter hook to suppress logging output for
  bdr message submissions

FUTURE WORK
---

- see if we can get bgworkers to show application_name in logs

- Split out pglogical_replication_set_add_table row filter / attribute filter
  setup code for re-use by BDR

- one day we'll need a "domain" for consensus messages, with per-domain
  counters, to support multiple nodegroups...

- some way to specify non-default interfaces for each node when joining

- make bdr_debug_level a function like
  bdr_debug_level(DEBUG2)
  that promotes anything <= the current bdr_debug_level setting to LOG
  so we can still filter out really noisy stuff

  and add a subsystem identifier so we can trace just messaging, for 
  example

- Make startup in the manager smarter and avoid polling the catalogs every pgl
  manager wait event wakeup. Maybe something like creating the manager's
  shmem segment directly from the nodegroup create and nodegroup join funcs,
  then telling it to wake up and notice?

SANITY CHECKS AND DEFENSES NEEDED
---

- In output plugin check that repsets subscribed-to are bdr sets

- Starting join should make node read-only

- Disallow consensus messages until part of a nodegroup

- Check pg_depend for relationship to local pglogical node
