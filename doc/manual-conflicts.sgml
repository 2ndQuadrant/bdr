<chapter id="conflicts" xreflabel="Multi-master conflicts">
 <title>Multi-master conflicts</title>

 <indexterm>
  <primary>Conflicts</primary>
 </indexterm>

 <para>
  In multi-master use of &bdr; writes to the same or related table(s) from multiple
  different nodes can result in data conflicts.
 </para>

 <para>
  Some clustering systems use distributed lock mechanisms to prevent
  concurrent access to data. These can perform reasonably when servers are
  very close but cannot support geographically distributed applications as
  very low latency is critical for acceptable performance.
 </para>

 <para>
  Distributed locking is essentially a pessimistic approach, whereas BDR
  advocates an optimistic approach: avoid conflicts where possible but allow
  some types of conflict to occur and and resolve them when they arise.
 </para>

 <sect1 id="conflicts-how" xreflabel="How conflicts happen">
  <title>How conflicts happen</title>

  <para>
   Inter-node conflicts arise as a result of sequences of events that could not
   happen if all the involved transactions happened concurrently on the same
   node. Because the nodes only exchange changes after transactions commit, each
   transaction is individually valid on the node it committed on but would not
   be valid if run on another node that has done other work in the mean time.
   Since &bdr; apply essentially replays the transaction on the other nodes, the
   replay operation can fail if there is a conflict between a transaction being
   applied and a transaction that was committed on the receiving node.
  </para>

  <para>
   The reason most conflicts can't happen when all transactions run on a single
   node is that PostgreSQL has inter-transaction communication mechanisms
   to prevent it - <literal>UNIQUE</literal> indexes,
   <literal>SEQUENCE</literal>s, row and relation locking,
   <literal>SERIALIZABLE</literal> dependency tracking, etc. All of these
   mechanisms are ways to communicate between transactions to prevent
   undesirable concurrency issues.
  </para>

  <para>
   &bdr; does not have a distributed transaction manager or lock manager.
   That's part of why it performs well with latency and network partitions. As
   a result, so <emphasis>transactions on different nodes execute entirely in
   isolation from each other</emphasis>. Despite the usual perception that
   "more isolation is good" you actually need to reduce isolation to prevent
   conflicts.
  </para>

  <sect2 id="conflicts-types" xreflabel="Types of conflict">
   <title>Types of conflict</title>

   <sect3 id="conflicts-key" xreflabel="PRIMARY KEY or UNIQUE conflicts">
    <title><literal>PRIMARY KEY</literal> or <literal>UNIQUE</literal> conflicts</title>

    <para>
     The most common conflicts are row conflicts where two operations affect a
     row with the same key in ways they could not do on a single node. &bdr; can
     detect most of those and apply last-update-wins conflict handling or invoke
     a user-defined conflict handler.
    </para>

    <para>
     Row conflicts include:
     <itemizedlist>
      <listitem><simpara><literal>INSERT</literal> vs <literal>INSERT</literal></simpara></listitem>
      <listitem><simpara><literal>INSERT</literal> vs <literal>UPDATE</literal></simpara></listitem>
      <listitem><simpara><literal>UPDATE</literal> vs <literal>DELETE</literal></simpara></listitem>
      <listitem><simpara><literal>INSERT</literal> vs <literal>DELETE</literal></simpara></listitem>
     </itemizedlist>
    </para>

    <sect4 id="conflicts-insert-unique"
           xreflabel="INSERTs that violate one UNIQUE constraint">

     <para>
      The most common conflict,  <literal>INSERT</literal> vs
      <literal>INSERT</literal>, arises where <literal>INSERT</literal>s on two
      different nodes create a tuple with the same <literal>PRIMARY
      KEY</literal> values (or the same values for a single
      <literal>UNIQUE</literal> constraint).  &bdr; handles this by retaining
      the most recently inserted tuple of the two according to the originating
      host's timestamps unless a user-defined conflict handler overrides this.
     </para>

     <para>
      No special administrator action is required to deal with these conflicts,
      but the user must undersand that <emphasis>one of the
      <literal>INSERT</literal>ed tuples is effectively discarded on all
      nodes</emphasis> - there is no data merging done unless a user defined
      conflict handler does it.
     </para>

    </sect4>

    <sect4 id="conflicts-insert-unique-multiple-index"
          xreflabel="INSERTs that violate multiple UNIQUE constraints">
      <title>INSERTs that violate multiple UNIQUE constraints</title>

     <para>
      An <literal>INSERT</literal>/<literal>INSERT</literal> conflict
      can violate more than one <literal>UNIQUE</literal> constraint
      (of which one might be the <literal>PRIMARY KEY</literal>).
     </para>

     <para>
      &bdr; can only handle an
      <literal>INSERT</literal>/<literal>INSERT</literal> conflict on one
      unique constraint (including the <literal>PRIMARY KEY</literal>). If a
      new row conflicts with more than one <literal>UNIQUE</literal> constraint
      then the apply worker that's trying to apply the change will
      <literal>ERROR</literal> out with:
      <programlisting>
      ERROR: multiple unique constraints violated by remotely INSERTed tuple
      </programlisting>
      (Older versions would report a <literal>"diverging uniqueness
      conflict"</literal> error instead).
     </para>

     <para>
      In case of such a conflict, you must manually remove the conflicting
      tuple(s) from the local side by <literal>DELETE</literal>ing it or by
      <literal>UPDATE</literal>ing it so that it no longer conflicts with the
      new remote tuple. There may be more than one conflicting tuple. There is
      not currently any built-in facility to ignore, discard or merge tuples
      that violate more than one local unique constraint.
     </para>

    </important>

   </sect3>

   <sect3 id="conflicts-foreign-key" xreflabel="Foreign key conflicts">
    <title>Constraint conflicts</title>

    <para>
     Conflicts between a remote transaction being applied and existing local data
     can also occur for <literal>FOREIGN KEY</literal> constraints. These
     conflicts are usually transient issues that arise from transactions being
     applied in a different order to the order they appeared to occur logically
     on the nodes that originated them.
    </para>

    <para>
     While apply is strictly ordered for any given origin node, there is no
     enforcement of ordering of transactions between two different nodes, so
     it's possible for (e.g.) node1 to insert a row into T1, which is replayed to node2.
     node2 inserts a row into T2 which has a foreign key reference to the row from T1.
     On node3, if the transaction from node2 that inserts the row into T2 is received
     <emphasis>before</emphasis> the transaction from node1 that inserts the row into T1,
     the transaction from node2 will fail to apply. This failure will record a rollback
     in <xref linkend="catalog-pg-stat-bdr"> and an <literal>ERROR</literal>
     with details in the PostgreSQL error log on the applying node (node3). In
     this case &bdr; will retry the transaction from node2 periodicially, so
     once it's replayed the transaction from node1 that it depends on the
     transaction will commit successfully.
    </para>

    <sect4 id="conflict-foreign-key-deadlock"
           xreflabel="Foreign key constraint deadlocks">
     <title>Foreign key constraint deadlocks</title>

    <para>
     Simple foreign key constraint conflicts are generally transient and
     require no administrator action, but for transactions that change multiple
     entities this is not always the case. It is possible for
     concurrent changes to tables with foreign key constraints to create
     <emphasis>inter-node replication deadlocks</emphasis> where no node can
     apply changes from any other node because they conflict with local data.
     This causes replication activity to stop until the deadlock is broken by a
     local data change on one or more of the nodes.
    </para>

    <para>
     For example, take a two node system with two tables and some existing data:
     <programlisting>
     CREATE TABLE parent(
      id integer primary key
     );

     CREATE TABLE child(
      id integer primary key,
      parent_id integer not null references parent(id)
     );

     INSERT INTO parent(id)
     VALUES (1), (2);

     INSERT INTO child(id, parent_id)
     VALUES (11, 1), (11, 2);
     </programlisting>
     If node A does:
     <programlisting>
     INSERT INTO child(id, parent_id)
     VALUES (21, 2);
     </programlisting>
     and <emphasis>at the same time</emphasis> node B does:
     <programlisting>
     DELETE FROM child WHERE parent_id = 2;
     DELETE FROM parent WHERE id = 2;
     </programlisting>
     then we have a situation where the transaction from node A cannot apply
     successfully to the <literal>child</literal> table on node B because the
     referenced <literal>parent</literal> no longer exists. The transaction
     from node B cannot apply to node A because it deletes a
     <literal>parent</literal> tuple that's still referenced, the new one with
     id=21. Neither transaction can replay, and both will output periodic
     <literal>ERROR</literal>s in the log files as they are retried. Since
     &bdr; replays transactions from a given node strictly in order, neither
     node can make progress with replication unless the user, or some 3rd node,
     makes changes that resolve the deadlock.
    </para>

    <para>
     It is important to note that when we manually deleted the child tuples
     on node B, the newly inserted child on node A was not affected because
     it had not yet replicated to node B. If either node replays the other's
     transaction before attempting its own local transaction then no problem
     will occur.
    </para>

    <para>
     Solving such a foreign key deadlock requires that you fix the constraint
     issue on each end. In this case, you would need to insert a dummy
     <literal>parent</literal> row on node B and delete the new child on node
     A. Replay will continue past the deadlock point.
    </para>

    <para>
     &bdr; can't just apply the changes from each end anyway because doing so
     would result in tables that violated their declared foreign key
     constraints, which most users would view as corruption.
    </para>

    </sect4>

   </sect3>

   <sect3 id="conflicts-exclusion"
          xreflabel="Exclusion constraint conflicts">
    <title>Exclusion constraint conflicts</title>

    <para>
     &bdr; doesn't support exclusion constraints and restricts their creation.
    </para>

    <important>
     <para>
      If an existing stand-alone database is converted to a &bdr; database then
      all exclusion constraints should be manually dropped.
     </para>
    </important>

    <para>
     In a distributed asynchronous system it is not possible to ensure that no
     set of rows that violates the constraint exists, because all transactions
     on different nodes are fully isolated. Exclusion constraints would lead to
     replay deadlocks where replay could not progress from any node to any
     other node because of exclusion constraint violations.
    </para>

    <para>
     If you force &bdr; to create an exclusion constraint, or you don't drop
     existing ones when converting a standalone database to &bdr; you should
     expect replication to break. You can get it to progress again by
     removing or altering the local tuple(s) that an incoming remote tuple
     conflicts with so that the remote transaction can be applied.
    </para>

   </sect3>

   <sect3>
    <title>Global data conflicts</title>

    <para>
     Conflicts can also arise where nodes have global (PostgreSQL-system-wide)
     data, like roles, that differs. This can result in operations - mainly
     <acronym>DDL</acronym> - that can be run successfully and committed
     on one node, but then fail to apply to other nodes.
    </para>

    <para>
     For example, node1 might have a user named
     <literal>fred</literal>, but that user was not created on node2.
     &bdr; does not replicate <literal>CREATE USER</literal> (see
     <xref linkend="ddl-create-role">) so this situation can arise easily.
     If <literal>fred</literal> on node1 creates a table, it will
     be replicated with its owner set to <literal>fred</literal>.
     When the DDL command is applied to node2 the DDL will fail
     because there is no user named <literal>fred</literal>.
     This failure will emit an <literal>ERROR</literal> in the
     PostgreSQL logs on node2 and increment
     <xref linkend="catalog-pg-stat-bdr"><literal>.nr_rollbacks</literal>.
    </para>

    <para>
     Administrator intervention is required to resolve this conflict
     by creating the user <literal>fred</literal> on node2.
     (It need not have the same permissions, but must exist).
    </para>

   </sect3>

   <sect3>
    <title>Lock conflicts and deadlock aborts</title>

    <para>
     Because &bdr; apply processes operate very like normal user sessions
     they are subject to the usual rules around row and table locking. This
     can sometimes lead to &bdr; apply processes waiting on locks held
     by user transactions, or even by each other.
    </para>

    <para>
     Relevant locking includes;
     <itemizedlist>
      <listitem><simpara>explicit table-level locking (<literal>LOCK TABLE ...</literal>) by user sessions</simpara></listitem>
      <listitem><simpara>explicit row level locking (<literal>SELECT ... FOR UPDATE/FOR SHARE</literal>) by user sessions</simpara></listitem>
      <listitem><simpara>locking from foreign keys</simpara></listitem>
      <listitem><simpara>implicit locking because of row <literal>UPDATE</literal>s, <literal>INSERT</literal>s or <literal>DELETE</literal>s, either from local activity or apply from other servers</simpara></listitem>
     </itemizedlist>
    </para>

    <para>
     It is even possible for a &bdr; apply process to deadlock with a user
     transaction, where the user transaction is waiting on a lock held
     by the apply process and vice versa. Two apply processes may also
     deadlock with each other. PostgreSQL's deadlock detector will
     step in and terminate one of the problem transactions. If the &bdr; apply
     worker's process is terminated it will simply retry and generally succeed.
    </para>

    <para>
     All these issues are transient and generally require no administrator
     action. If an apply process is stuck for a long time behind a lock
     on an idle user session the administrator may choose to terminate
     the user session to get replication flowing again, but this is
     no different to a user holding a long lock that impacts another
     user session.
    </para>

    <para>
     Use of the <ulink
     url="http://www.postgresql.org/docs/current/static/runtime-config-logging.html#GUC-LOG-LOCK-WAITS">
     log_lock_waits</ulink> facility in PostgreSQL can help identify locking
     related replay stalls.
    </para>

   </sect3>

   <sect3>
    <title>Divergent conflicts</title>

    <para>
     Divergent conflicts arise when data that should be the same on different
     nodes differs unexpectedly. Divergent conflicts should not occur, but not
     all such conflicts can be reliably prevented at time of writing.
    </para>

    <warning>
     <para>
      Changing the <literal>PRIMARY KEY</literal> of a row can lead to a
      divergent conflict if another node changes the key of the same row before
      all nodes have replayed the change. Avoid changing primary keys, or
      change them only on one designated node.
     </para>
    </warning>

    <para>
     Divergent conflicts involving row data generally require administrator
     action to manually adjust the data on one of the nodes to be consistent
     with the other one while replication is temporarily disabled using <xref
     linkend="guc-bdr-do-not-replicate">. Such conflicts should not arise
     so long as &bdr; is used as documented and settings or functions marked
     as unsafe are avoided.
    </para>

    <!-- TODO: how do we do this? -->

   </sect3>

  </sect2>

 </sect1>

 <sect1 id="conflicts-avoidance" xreflabel="Conflict avoidance">
  <title>Avoiding or tolerating conflicts</title>

  <para>
   In most cases appropriate application design can be used to avoid conflicts
   and/or the application can be made tolerant of conflicts.
  </para>

  <para>
   Conflicts can only happen if there are things happening at the same time on
   multiple nodes, so the simplest way to avoid conflicts is to only ever write
   to one node, or to only ever write to independent subsets of the database on
   each node. For example, each node might have a separate schema, and while
   they all exchange data with each other, writes are only ever performed on
   the node that "owns" a given schema.
  </para>

  <para>
   For <literal>INSERT</literal> vs <literal>INSERT</literal> conflicts, use of
   <xref linkend="global-sequences"> can completely prevent conflicts.
  </para>

  <para>
   BDR users may sometimes find it useful to perform distributed locking at the
   application level in cases where conflicts are not acceptable.
  </para>

  <para>
   The best course of action is frequently to allow conflicts to occur and
   design the application to work with &bdr;'s conflict resolution
   mechansisms to cope with the conflict. See <xref linkend="conflicts-resolution">.
  </para>

 </sect1>

 <sect1 id="conflicts-resolution" xreflabel="Conflict resolution">
  <title>Conflict resolution</title>

  <para>
   When &bdr; detects data row conflicts it ensures that the conflict is
   handled the same way on all nodes, so that the resulting state of the table
   is the same for every node.
  </para>

  <para>
   By default &bdr; uses a <emphasis>last-update-wins</emphasis> strategy
   to resolve conflicts. The most recent update will be retained,
   and the oldest update discarded in its entirety. No attempt to merge
   the data in the conflicting rows is made.
  </para>

  <para>
   Users may override this behaviour with application-specific knowledge
   in <xref linkend="conflicts-user-defined-handlers">.
  </para>

  <!-- TODO -->

 </sect1>

 <sect1 id="conflicts-user-defined-handlers" xreflabel="User defined conflict handlers">
  <title>User defined conflict handlers</title>

  <para>
   &bdr; provides facilities for users to override the default last-update-wins
   data row conflict resolution strategy.
  </para>

  <!-- TODO -->

  <para>
   See also: <xref linkend="functions-conflict-handlers">
  </para>

 </sect1>

 <sect1 id="conflicts-logging" xreflabel="Conflict logging">
  <title>Conflict logging</title>

  <para>
   To make diagnosis and handling of multi-master conflicts easier, &bdr;
   supports logging of each conflict incident in a <xref linkend="catalog-bdr-conflict-history"> table.
  </para>

  <para>
   Conflict logging is only enabled when <xref
   linkend="guc-bdr-conflict-logging-include-tuples"> is
   <literal>true</literal>.
  </para>

  <para>
   You can use the conflict history table to determine how rapidly your
   application creates conflicts and where those conflicts occur, allowing you to
   improve the application to reduce conflict rates. It also helps detect cases
   where conflict resolutions may not have produced the desired results, allowing
   you to identify places where a user defined conflict trigger or an application
   design change may be desirable.
  </para>

  <para>
   Row values may optionally be logged for row conflicts. This is controlled by
   the global database-wide option <xref linkend="guc-bdr-log-conflicts-to-table">.
   There is no per-table control over row value logging at this time. Nor is
   there any limit applied on the number of fields a row may have, number of
   elements dumped in arrays, length of fields, etc, so it may not be wise to
   enable this if you regularly work with multi-megabyte rows that may trigger
   conflicts.
  </para>

  <para>
   Because the conflict history table contains data on every table in the
   database so each row's schema might be different, if row values are logged
   they are stored as json fields. The json is created with
   <function>row_to_json</function>, just like if you'd called it on the row
   yourself from SQL. There is no corresponding
   <function>json_to_row</function> function in PostgreSQL at this time, so
   you'll need table-specific code (pl/pgsql, pl/python, pl/perl, whatever) if
   you want to reconstruct a composite-typed tuple from the logged json.
  </para>

 </sect1>

</chapter>
