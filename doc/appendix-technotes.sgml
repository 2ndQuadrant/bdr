<appendix id="technotes" xreflabel="Technical notes">
 <title>Technical notes</title>

 <para>
  Additional techncial and implementation detail on some BDR behaviour and
  requirements is carried in this appendix.
 </para>

 <sect1 id="technotes-mesh" xreflabel="BDR mesh network">
  <title>BDR network structure</>

  <para>
   BDR uses a mesh topology, where every node can communicate directly with
   every other node. It doesn't support circular replication, forwarding,
   cascading, etc.
  </para>

  <para>
   Each pair of nodes communicates over a pair of (mostly) uni-directional
   channels, one to stream data from node A=>B and one to stream data from node
   B=>A. This means each node must be able to connect directly to each other
   node. Firewalls, NAT, etc must be configured accordingly.
  </para>
  
  <para>
   Every BDR node must have a <ulink
   url="https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html">replication
   slot</> on every other BDR node so it can replay changes from the node, and
   every node must have a replication origin for each other node so it can keep
   track of replay progress.  If nodes were allowed to join while another was
   offline or unreachable due to a network partition, it would have no way to
   replay any changes made on that node and the BDR group would get out of
   sync. Since bdr does no change forwarding during normal operation, that
   desynchronisation would not get fixed.
  </para>

  <para>
   The addition of enhanced change forwarding support could allow for cascading
   nodes isolated from the rest of the mesh, allow new nodes to join and lazily
   switch over to directly receiving data from a node when it becomes
   reachable, etc. It's not fundamentally necessary for all nodes to be
   reachable during node join, it's just a requirement for the current
   implementation. There's already limited change forwarding support
   in place and used for initial node clone.
  </para>

  <para>
   DDL locking enhancements would also be required; see <xref linkend="ddl-replication">
   and <xref linkend="technotes-ddl-locking">.
  </para>

 </sect1>

 <sect1 id="technotes-ddl-locking" xreflabel="DDL locking details">
  <title>DDL locking details</>

  <para>
   To ensure complete consistency of some types of schema change operations
   (DDL), BDR must be able to sometimes go into fully synchronous mode, where
   all nodes flush all pending changes to each other, replay a change, and
   confirm that change before any of them can proceed with new work. See
   <xref linkend="ddl-replication">. This also means all nodes must be reachable,
   and it means that if we add a new node it must be impossible for existing
   nodes that are currently down or unreachable to gain a DDL lock and make
   schema changes until they can communicate with the new node. This would
   require extra inter-node communication and DDL locking protocol
   enhancements.
  </para>

  <para>
   If BDR didn't go synchronous for schema changes, multiple nodes could make
   conflicting schema changes. Worse, outstanding changes for the old format of
   a table might not make sense when arriving at a node that has the new format
   for a table. For example, the new table might have added a new not-null
   column, but the incoming row doesn't have a value for it.  More complex
   cases also exist, and there's no simple resolution to all such possible
   problems.
  </para>

  <para>
   Some optimisations have already been made here. In particular, DDL that
   won't cause apply conflicts only takes a weaker lock mode that doesn't block
   writes. The weaker DDL lock mode also allows locking to proceed without
   every server handshaking to every other server; it only needs the requesting
   server to communicate with all its peers, not transitively with their peers
   in turn. Only the DDL write lock now requires that all nodes confirm that
   they have flushed all pending transactions to all other nodes.
  </para>

  <sect2 id="ddl-replication-locking-howworks" xreflabel="How DDL locking works">
   <title>How the DDL lock works</title>

   <para>
    If you don't care how the global DDL lock works you can skip this section,
    but understanding it will be useful when you're diagnosing issues.
   </para>

   <para>
    There are two levels to the DDL lock: the global DDL lock, which only one node
    at a time may hold, and the local DDL lock, which each node has separately.
    When the global DDL lock is held then all the local DDL locks are held too.
   </para>

   <para>
    Inter-node communication is done via WAL messages written to the
    transaction logs and replayed by apply workers. So replication and replay
    lag will result in lag with DDL locking too.
   </para>

   <para>
    There are also two (currently) DDL lock modes. The weak 'ddl' lock, and the
    'write' lock. The global 'ddl' mode prevents other nodes from running any
    DDL while it is held by holding each node's local DDL lock. The 'write'
    mode further requires that all nodes complete all in-progress transactions,
    disallow new write operations, and make sure they have fully replayed all
    changes from their peers. BDR versions prior to 1.0 only had the
    heavier-weight 'write' mode lock.
   </para>

   <para>
    The (somewhat simplified) process of DDL lock acquision is:
   </para>

   <orderedlist>
    <listitem><para>A normal user backend attempts something that requires the DDL lock</para></listitem>
    <listitem><para>The BDR command filter notices that the DDL lock is needed, pauses the user's command, and requests that the local BDR node acquire the global DDL lock</para></listitem>
    <listitem><para>The local BDR node acquires its own local DDL lock. It will now reject any incoming lock requests from other nodes, cancel write transactions if needed, and pause new write transactions if needed.</para></listitem>
    <listitem><para>The local DDL node writes a message in its replicaiton stream to ask every other node to take their local DDL locks and reply to confirm they've done so</para></listitem>
    <listitem><para>Every node that gets the request acquires the local DDL lock to prevent concurrent DDL and possibly writes, then replies to the requestor to confirm that its lock DDL lock is taken.</para></listitem>
    <listitem><para>When all peers have confirmed lock acquisition, the requesting node knows it now holds the global DDL lock. If it's acquiring a weak DDL lock it's done now. If it's acquiring a write lock it must wait until all peers confirm replay.</para></listitem>
    <listitem><para>If the DDL lock request was a write-lock request, each node receiving a lock request checks with every other node to see that they've all replayed all outstanding changes from each other and waits for them all to reply with confirmation, then sends its own catchup confirmation.</para></listitem>
    <listitem><para>Once it has the global DDL lock, and (for write locks) knows all peers are caught up to each other, the requesting node is ready to proceed.</para></listitem>
    <listitem><para>The requesting node makes the required schema changes</para></listitem>
    <listitem><para>The requesting node writes the fact that it's done with the DDL lock to its WAL in the form of a lock release message</para></listitem>
    <listitem><para>The requesting node releases its local DDL lock and resumes normal write operations</para></listitem>
    <listitem><para>The other nodes replay the lock release message and release their local DDL locks, resuming normal write operations</para></listitem>
   </orderedlist>

   <para>
    Critically, this means that for write-locks <emphasis>every BDR node must
    complete a two-way communication with every other BDR node before the DDL
    lock can be granted</emphasis>. This communication is done via the
    replication stream, so replication lag and delays, network slowness or
    outages, etc in turn delay the DDL locking process. While the system is in
    the process of acquiring the DDL lock, many nodes will hold their local DDL
    locks and will be rejecting other DDL requests or, if the lock mode
    requires, rejecting writes.
   </para>

   <para>
    This means that schema changes and anything else that takes the DDL
    lock should only be performed when all nodes are reachable and there
    isn't a big replication delay between any pair of nodes.
   </para>

   <para>
    It also means that if the system gets stuck waiting for a down node,
    everything stops while we wait.
   </para>
   
   <para>
    If the DDL lock request is canceled by the requesting node, all the other
    reachable nodes will release their locks. So if your system is hung up
    on a DDL lock request that's making no progress you can just cancel the
    statement that's requesting the DDL lock and everything will resume normal
    operation.
   </para>

   <para>
    Full details can be found in the comments on
    <filename>bdr_locks.c</filename>.
   </para>

  </sect2>

 </sect1>

 <sect1 id="technotes-rewrites" xreflabel="Full table rewrites">
  <title>Full table rewrites</>

  <para>
   There are a number of reasons why BDR doesn't support DDL operations that
   perform full table rewrites.
  </para>

  <para>
   They tend to be very slow operations for which the <link
   linkend="ddl-replication-locking"> global DDL lock</link> must be held
   throughout. That's a long time to "stop the world".  They can be problematic
   for apps on standalone PostgreSQL for this reason, but it's worse on BDR due
   to the global DDL lock.
  </para>

  <para>
   Table rewrites discard replication origin and commit timestamp information
   that we need to ensure that conflict resolution is consistent across all
   nodes. There's currently no way to remap it.
  </para>

  <para>
   Finally, we can't guarantee that the rewrite will have the same results
   across all nodes unless the entire expression it uses is classified as
   immutable. This isn't currently checked for. Even seemingly safe
   defaults like <function>nextval(...)</function> aren't safe because
   the order in which rows are processed by a table rewrite will be
   different on different nodes, so different rows will get a given
   generated value on each node.
  </para>

  <para>
   Because of the performance issues we recommend that table-rewriting
   operations be split up into multiple smaller operations and aren't
   prioritizing support for the subset of them that can be made safe; see
   <xref linkend="ddl-replication-how">.
  </para>

 </sect1>

</appendix>
